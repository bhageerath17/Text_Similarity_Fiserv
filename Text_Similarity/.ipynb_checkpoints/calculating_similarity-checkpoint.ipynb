{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Similarity\n",
    "\n",
    "First, let's create some embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Today's consumers want options that help them take control of their financial lives on the go. CardValet from Fiserv is a mobile app that allows financial institutions to deliver the experiences consumers expect by enabling them to define when, where and how their cards are used.\",\n",
    "    \"Consumerâ€™s want options that let them take control of their financial lives on the go. CardValet from Fiserv is a mobile app that helps banks deliver consumer experiences by enabling them to define when, where and how their cards are used.\",\n",
    "    \"CardValet from Fiserv is a mobile app that consumers expect them to enable their cards and define when their cards are used. CardValet can be used as A branded app that can be integrated via single sign-on with your mobile banking platform giving cardholders a seamless banking experience\",\n",
    "    \"Standing on one's head at job interviews forms a lasting impression.\"   \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sentence-transformers/bert-base-nli-mean-tokens were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/bert-base-nli-mean-tokens')\n",
    "\n",
    "# initialize dictionary that will contain tokenized sentences\n",
    "tokens = {'input_ids': [], 'attention_mask': []}\n",
    "\n",
    "for sentence in sentences:\n",
    "    # tokenize sentence and append to dictionary lists\n",
    "    new_tokens = tokenizer.encode_plus(sentence, max_length=128, truncation=True,\n",
    "                                       padding='max_length', return_tensors='pt')\n",
    "    tokens['input_ids'].append(new_tokens['input_ids'][0])\n",
    "    tokens['attention_mask'].append(new_tokens['attention_mask'][0])\n",
    "\n",
    "# reformat list of tensors into single tensor\n",
    "tokens['input_ids'] = torch.stack(tokens['input_ids'])\n",
    "tokens['attention_mask'] = torch.stack(tokens['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We process these tokens through our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(**tokens)\n",
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dense vector representations of our `text` are contained within the `outputs` **'last_hidden_state'** tensor, which we access like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.9276e-01, -2.6861e-02,  1.2145e+00,  ..., -8.5463e-01,\n",
       "          -1.3567e+00,  5.0336e-01],\n",
       "         [-9.2307e-01,  2.2581e-01,  1.4719e+00,  ..., -1.3455e+00,\n",
       "          -1.0756e+00,  4.2485e-02],\n",
       "         [-3.9946e-01, -2.4946e-01,  1.9602e+00,  ..., -8.4858e-01,\n",
       "          -1.1761e+00,  2.4284e-01],\n",
       "         ...,\n",
       "         [-2.5001e-01, -2.5791e-01,  8.8925e-01,  ..., -4.5543e-01,\n",
       "          -1.1812e+00,  1.9731e-01],\n",
       "         [-1.3332e-01, -1.6209e-01,  8.7017e-01,  ..., -4.4412e-01,\n",
       "          -1.1705e+00,  2.5546e-01],\n",
       "         [-1.1183e-01, -2.8377e-01,  8.6390e-01,  ..., -4.0031e-01,\n",
       "          -1.2038e+00,  2.4586e-01]],\n",
       "\n",
       "        [[-1.5972e-01,  1.4743e-01,  7.8997e-01,  ..., -9.1521e-01,\n",
       "          -1.6520e+00,  5.4084e-01],\n",
       "         [-3.3597e-01,  2.4327e-01,  1.1927e+00,  ..., -1.0822e+00,\n",
       "          -1.4735e+00,  1.3521e-01],\n",
       "         [ 3.1547e-01,  5.4656e-02,  9.1885e-01,  ..., -8.4436e-01,\n",
       "          -1.1599e+00,  3.6451e-01],\n",
       "         ...,\n",
       "         [-1.3240e-01, -1.9676e-02,  3.8525e-01,  ..., -3.8466e-01,\n",
       "          -1.1339e+00,  2.4669e-01],\n",
       "         [-1.3525e-01, -9.6533e-03,  3.8596e-01,  ..., -3.8202e-01,\n",
       "          -1.1428e+00,  2.1925e-01],\n",
       "         [-1.1220e-01,  3.1012e-04,  4.1028e-01,  ..., -3.9858e-01,\n",
       "          -1.1932e+00,  2.3553e-01]],\n",
       "\n",
       "        [[-1.9373e-01,  3.9518e-01,  5.6032e-01,  ..., -7.8469e-01,\n",
       "          -1.2638e+00,  4.3238e-01],\n",
       "         [-2.2069e-01, -4.8562e-01,  1.4179e+00,  ..., -1.1611e-01,\n",
       "          -1.0912e+00,  4.4750e-01],\n",
       "         [ 7.3472e-01, -1.6365e-01,  9.1406e-01,  ..., -2.1915e-01,\n",
       "          -1.2251e+00, -3.5539e-01],\n",
       "         ...,\n",
       "         [-1.1978e-02, -8.9322e-02,  5.9721e-01,  ..., -3.0258e-01,\n",
       "          -1.4208e+00,  3.0774e-01],\n",
       "         [-4.6436e-02, -1.3822e-01,  5.7431e-01,  ..., -3.5826e-01,\n",
       "          -1.3399e+00,  2.7177e-01],\n",
       "         [-2.1272e-01, -1.4641e-01,  5.9582e-01,  ..., -3.2523e-01,\n",
       "          -1.2572e+00,  2.7904e-01]],\n",
       "\n",
       "        [[-1.0246e-01,  9.7842e-01,  1.4798e+00,  ..., -6.7322e-01,\n",
       "          -1.3459e+00, -1.5414e-01],\n",
       "         [ 1.6459e-01,  1.1261e+00,  9.7448e-01,  ..., -8.2403e-01,\n",
       "          -1.5562e+00, -6.0396e-01],\n",
       "         [ 4.7917e-01,  9.7228e-01,  1.3746e+00,  ..., -9.8250e-01,\n",
       "          -1.3523e+00, -5.8834e-01],\n",
       "         ...,\n",
       "         [ 6.3124e-02,  3.3896e-01,  1.2718e+00,  ..., -3.9970e-01,\n",
       "          -1.1031e+00, -1.3408e-01],\n",
       "         [ 1.3678e-01,  4.4807e-01,  1.2677e+00,  ..., -3.7586e-01,\n",
       "          -1.0867e+00, -2.6921e-01],\n",
       "         [ 1.4712e-01,  3.7091e-01,  1.2411e+00,  ..., -3.6103e-01,\n",
       "          -1.1337e+00, -2.6628e-01]]], grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = outputs.last_hidden_state\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have produced our dense vectors `embeddings`, we need to perform a *mean pooling* operation on them to create a single vector encoding (the **sentence embedding**). To do this mean pooling operation we will need to multiply each value in our `embeddings` tensor by it's respective `attention_mask` value - so that we ignore non-real tokens.\n",
    "\n",
    "To perform this operation, we first resize our `attention_mask` tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = tokens['attention_mask']\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = attention_mask.unsqueeze(-1).expand(embeddings.size()).float()\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each vector above represents a single token attention mask - each token now has a vector of size 768 representing it's *attention_mask* status. Then we multiply the two tensors to apply the attention mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 128, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_embeddings = embeddings * mask\n",
    "masked_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2928, -0.0269,  1.2145,  ..., -0.8546, -1.3567,  0.5034],\n",
       "         [-0.9231,  0.2258,  1.4719,  ..., -1.3455, -1.0756,  0.0425],\n",
       "         [-0.3995, -0.2495,  1.9602,  ..., -0.8486, -1.1761,  0.2428],\n",
       "         ...,\n",
       "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.1597,  0.1474,  0.7900,  ..., -0.9152, -1.6520,  0.5408],\n",
       "         [-0.3360,  0.2433,  1.1927,  ..., -1.0822, -1.4735,  0.1352],\n",
       "         [ 0.3155,  0.0547,  0.9188,  ..., -0.8444, -1.1599,  0.3645],\n",
       "         ...,\n",
       "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "         [-0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.1937,  0.3952,  0.5603,  ..., -0.7847, -1.2638,  0.4324],\n",
       "         [-0.2207, -0.4856,  1.4179,  ..., -0.1161, -1.0912,  0.4475],\n",
       "         [ 0.7347, -0.1637,  0.9141,  ..., -0.2191, -1.2251, -0.3554],\n",
       "         ...,\n",
       "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000],\n",
       "         [-0.0000, -0.0000,  0.0000,  ..., -0.0000, -0.0000,  0.0000]],\n",
       "\n",
       "        [[-0.1025,  0.9784,  1.4798,  ..., -0.6732, -1.3459, -0.1541],\n",
       "         [ 0.1646,  1.1261,  0.9745,  ..., -0.8240, -1.5562, -0.6040],\n",
       "         [ 0.4792,  0.9723,  1.3746,  ..., -0.9825, -1.3523, -0.5883],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0000, -0.0000, -0.0000]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we sum the remained of the embeddings along axis `1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed = torch.sum(masked_embeddings, 1)\n",
    "summed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then sum the number of values that must be given attention in each position of the tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 768])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed_mask = torch.clamp(mask.sum(1), min=1e-9)\n",
    "summed_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[57., 57., 57.,  ..., 57., 57., 57.],\n",
       "        [52., 52., 52.,  ..., 52., 52., 52.],\n",
       "        [61., 61., 61.,  ..., 61., 61., 61.],\n",
       "        [16., 16., 16.,  ..., 16., 16., 16.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summed_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we calculate the mean as the sum of the embedding activations `summed` divided by the number of values that should be given attention in each position `summed_mask`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pooled = summed / summed_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3085,  0.1475,  1.3810,  ..., -0.8949, -1.3626,  0.4667],\n",
       "        [-0.1856,  0.2541,  1.1287,  ..., -0.9815, -1.5834,  0.5147],\n",
       "        [-0.0906,  0.4047,  0.8533,  ..., -0.5820, -1.4343,  0.3572],\n",
       "        [-0.0132,  0.9773,  1.4516,  ..., -0.8462, -1.4004, -0.4118]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pooled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is how we calculate our dense similarity vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate cosine similarity for sentence `0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9675461 , 0.90401113, 0.4043299 ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert from PyTorch tensor to numpy array\n",
    "mean_pooled = mean_pooled.detach().numpy()\n",
    "\n",
    "# calculate\n",
    "cosine_similarity(\n",
    "    [mean_pooled[0]],\n",
    "    mean_pooled[1:]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
